{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac15175-2d8b-4d91-b346-5f09952cd6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/30 17:10:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/30 17:10:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Familia RAW (después de Extract) ===\n",
      "+----------------+-----------+---------+----------+---------+-------------+-----------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+----------+-----------+----------+-----------+---------+-----------+-----------+-----------+----------+-----------+-----------+-----------+----------+-----------+------------+-----------+-----------+----------+----------------+--------------+---------------+--------------+-------------+-----------+------------+-----------+---------------+-------------+--------------+-------------+---------------+-------------+--------------+-------------+-------------+-----------+------------+-----------+\n",
      "|Producto_Familia|Enero_Venta|Enero_TGT|Enero_PY24|Enero_pct|Febrero_Venta|Febrero_TGT|Febrero_PY24|Febrero_pct|Marzo_Venta|Marzo_TGT  |Marzo_PY24|Marzo_pct  |Abril_Venta|Abril_TGT  |Abril_PY24|Abril_pct  |Mayo_Venta|Mayo_TGT   |Mayo_PY24|Mayo_pct   |Junio_Venta|Junio_TGT  |Junio_PY24|Junio_pct  |Julio_Venta|Julio_TGT  |Julio_PY24|Julio_pct  |Agosto_Venta|Agosto_TGT |Agosto_PY24|Agosto_pct|Septiembre_Venta|Septiembre_TGT|Septiembre_PY24|Septiembre_pct|Octubre_Venta|Octubre_TGT|Octubre_PY24|Octubre_pct|Noviembre_Venta|Noviembre_TGT|Noviembre_PY24|Noviembre_pct|Diciembre_Venta|Diciembre_TGT|Diciembre_PY24|Diciembre_pct|YTD_JUL_Venta|YTD_JUL_TGT|YTD_JUL_PY24|YTD_JUL_pct|\n",
      "+----------------+-----------+---------+----------+---------+-------------+-----------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+----------+-----------+----------+-----------+---------+-----------+-----------+-----------+----------+-----------+-----------+-----------+----------+-----------+------------+-----------+-----------+----------+----------------+--------------+---------------+--------------+-------------+-----------+------------+-----------+---------------+-------------+--------------+-------------+---------------+-------------+--------------+-------------+-------------+-----------+------------+-----------+\n",
      "|ELIPTIC FAM     |75101.55   |75101.55 |17741.06  |1        |84288.73     |107789.2834|19544.51    |0.781976903|61213.98   |107789.2834|19544.51  |0.567904137|51806.79   |113627.5028|33278.04  |0.455935304|60995.68  |113627.5028|14473.58 |0.536803841|61069.66   |113627.5028|18571.16  |0.537454916|10218.55   |125303.9417|21877.87  |0.081550108|0           |3448.722727|31060.62   |0         |0               |3448.722727   |30695.48       |0             |0            |3609.272727|24297.2     |0          |0              |3609.272727  |45634.51      |0            |0              |3609.272727  |52246.26      |0            |404694.94    |756866.5668|145030.73   |0.534697869|\n",
      "|AGGLAD          |110094.81  |110094.81|22534.35  |1        |110784.62    |114546.1364|31785.06    |0.96716156 |81102.53   |114546.1364|31785.06  |0.708033745|91519.9    |120891.2407|24635.46  |0.757043269|80999.82  |120891.2407|26193.67 |0.670022241|92445.3    |120891.2407|20271.95  |0.764698083|18240.65   |133581.4492|29784.66  |0.136550772|0           |2560.845455|25769.42   |0         |0               |2560.845455   |32943.44       |0             |0            |2681.545455|20463.2     |0          |0              |2681.545455  |52414.29      |0            |0              |2681.545455  |71802         |0            |585187.63    |835442.2541|186990.21   |0.700452517|\n",
      "|GAAP FAM        |198788.44  |198788.44|169078.66 |1        |160505.48    |189336.242 |165560.48   |0.847727188|150621.08  |189336.242 |165560.48 |0.795521652|105247.15  |199898.6823|162597.45 |0.52650247 |125926.36 |199898.6823|189372.95|0.629950926|157333.82  |199898.6823|155690.3  |0.78706782 |32603.91   |221023.5628|194520.31 |0.147513277|0           |2823.254545|200913.89  |0         |0               |2823.254545   |170748.45      |0             |0            |2957.654545|166393.8    |0          |0              |2957.654545  |187016.44     |0            |0              |2956.454545  |136807.4      |0            |931026.24    |1398180.534|1202380.63  |0.665884138|\n",
      "|LAGRICEL FAM    |377004.34  |377004.34|395594.16 |1        |413613.96    |414081.2322|439211.52   |0.998871545|447456.98  |414081.2322|439211.52 |1.080601933|456361.02  |436918.8275|375415.05 |1.044498409|348233.36 |436918.8275|357482.49|0.797020724|386762.05  |436918.8275|312000.17 |0.885203442|66352.14   |482594.018 |390711.15 |0.137490598|0           |9579.386364|444312.92  |0         |0               |9579.386364   |364618.85      |0             |0            |10033.63636|432340.84   |0          |0              |10033.63636  |413718.39     |0            |0              |10033.63636  |287199.94     |0            |2495783.85   |2998517.305|2709626.06  |0.832339319|\n",
      "|SOPHIPREN       |50525.9    |50525.9  |41450.28  |1        |73566.85     |56555.62783|39394.05    |1.300787434|57583.91   |56555.62783|39394.05  |1.018181783|67175.27   |59671.10342|51339.43  |1.125758804|56544.48  |59671.10342|43314.92 |0.947602386|63975.49   |59671.10342|48676.87  |1.072135193|12129.07   |65902.05458|49954.18  |0.184046917|0           |1649.890909|80483.82   |0         |0               |1649.890909   |63223.94       |0             |0            |1728.090909|50446.88    |0          |0              |1728.090909  |75069.98      |0            |0              |1728.090909  |38645.41      |0            |381500.97    |408552.5205|313523.78   |0.933786847|\n",
      "+----------------+-----------+---------+----------+---------+-------------+-----------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+----------+-----------+----------+-----------+---------+-----------+-----------+-----------+----------+-----------+-----------+-----------+----------+-----------+------------+-----------+-----------+----------+----------------+--------------+---------------+--------------+-------------+-----------+------------+-----------+---------------+-------------+--------------+-------------+---------------+-------------+--------------+-------------+-------------+-----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "import re\n",
    "\n",
    "# ==============================\n",
    "# 1. Inicializar Spark Session\n",
    "# ==============================\n",
    "# (E - Extract: La conexión a la fuente de datos)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HDFS_NiFi_Data_Cleaning_ETL\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ==============================\n",
    "# 2. Paths en HDFS (Fuente)\n",
    "# ==============================\n",
    "file_familia = \"/user/nifi/Resumen_Valores-VENTA_POR_FAMILIA2.csv\"\n",
    "file_producto = \"/user/nifi/Resumen _Valores-VENTA_POR-PRODUCTO2.csv\"\n",
    "\n",
    "# ==============================\n",
    "# 3. Función de Asignación de Nombres (Helper para E)\n",
    "# ==============================\n",
    "def rename_month_columns(df):\n",
    "    \"\"\"\n",
    "    Renombra los bloques de columnas por mes (Venta, TGT, PY24, pct)\n",
    "    a su formato final Mes_Métrica.\n",
    "    \"\"\"\n",
    "    meses = [\"Enero\",\"Febrero\",\"Marzo\",\"Abril\",\"Mayo\",\"Junio\",\n",
    "             \"Julio\",\"Agosto\",\"Septiembre\",\"Octubre\",\"Noviembre\",\"Diciembre\",\"YTD_JUL\"]\n",
    "    metricas = [\"Venta\",\"TGT\",\"PY24\",\"pct\"]\n",
    "\n",
    "    new_cols = [\"Producto_Familia\"]\n",
    "    col_idx = 1\n",
    "\n",
    "    for mes in meses:\n",
    "        for metrica in metricas:\n",
    "            if col_idx < len(df.columns):\n",
    "                new_cols.append(f\"{mes}_{metrica}\")\n",
    "                col_idx += 1\n",
    "\n",
    "    while len(new_cols) < len(df.columns):\n",
    "        new_cols.append(f\"Extra_{len(new_cols)}\")\n",
    "\n",
    "    return df.toDF(*new_cols)\n",
    "\n",
    "# ==============================\n",
    "# 4. Función de Extracción Principal\n",
    "# ==============================\n",
    "def extract_data(path):\n",
    "    \"\"\"\n",
    "    Lee el CSV, identifica la cabecera real, la filtra y asigna\n",
    "    nombres temporales a las columnas.\n",
    "    \"\"\"\n",
    "    # 1. Leer CSV sin header\n",
    "    df = spark.read.option(\"sep\", \";\").option(\"header\", \"false\").csv(path)\n",
    "\n",
    "    # 2. Tomar primera fila como header real\n",
    "    header_row = df.first()\n",
    "\n",
    "    # 3. Filtrar esa fila del dataframe (limpieza mínima)\n",
    "    df = df.filter(col(\"_c0\") != header_row[0])\n",
    "\n",
    "    # 4. Columnas temporales\n",
    "    tmp_headers = [f\"Col_{i}\" for i in range(len(df.columns))]\n",
    "    df = df.toDF(*tmp_headers)\n",
    "\n",
    "    # 5. Renombrar columnas a Mes_Métrica\n",
    "    df = rename_month_columns(df)\n",
    "    \n",
    "    # 6. Quitar la fila \"Producto\" (limpieza inicial)\n",
    "    df = df.filter(df[\"Producto_Familia\"] != \"Producto\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ==============================\n",
    "# 5. Ejecución del Extract\n",
    "# ==============================\n",
    "df_familia_raw = extract_data(file_familia)\n",
    "df_producto_raw = extract_data(file_producto)\n",
    "\n",
    "# Muestra inicial para verificación (Opcional)\n",
    "print(\"=== Familia RAW (después de Extract) ===\")\n",
    "df_familia_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff05e1d-f661-4364-9874-5c5d286812b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Resultado Unificado y Limpio (después de Transform) ===\n",
      "+----------------+---------+--------+-----------+---------+-----------+-------+\n",
      "|Producto_Familia|Mes      |PY24    |TGT        |Venta    |pct        |Nivel  |\n",
      "+----------------+---------+--------+-----------+---------+-----------+-------+\n",
      "|AGGLAD          |ABRIL    |24635.46|120891.2407|91519.9  |0.757043269|FAMILIA|\n",
      "|AGGLAD          |AGOSTO   |25769.42|2560.845455|0.0      |0.0        |FAMILIA|\n",
      "|AGGLAD          |DICIEMBRE|71802.0 |2681.545455|0.0      |0.0        |FAMILIA|\n",
      "|AGGLAD          |ENERO    |22534.35|110094.81  |110094.81|1.0        |FAMILIA|\n",
      "|AGGLAD          |FEBRERO  |31785.06|114546.1364|110784.62|0.96716156 |FAMILIA|\n",
      "+----------------+---------+--------+-----------+---------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Producto_Familia: string (nullable = true)\n",
      " |-- Mes: string (nullable = true)\n",
      " |-- PY24: double (nullable = true)\n",
      " |-- TGT: double (nullable = true)\n",
      " |-- Venta: double (nullable = true)\n",
      " |-- pct: double (nullable = true)\n",
      " |-- Nivel: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, regexp_extract, first, trim, upper, when\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ==============================\n",
    "# 1. Función de Transformación Principal\n",
    "# ==============================\n",
    "def transform_to_wide_format(df, nivel_valor):\n",
    "    \"\"\"\n",
    "    Aplica todas las transformaciones: unpivot, pivot, tipado, normalización\n",
    "    para pasar de formato ancho (Mes_Metrica) a formato largo (Mes, Metrica).\n",
    "    \"\"\"\n",
    "    id_col = \"Producto_Familia\"\n",
    "    value_cols = [c for c in df.columns if c != id_col]\n",
    "\n",
    "    # 1. Unpivot (formato largo intermedio)\n",
    "    expr_str = \"stack({0}, {1}) as (Columna, Valor)\".format(\n",
    "        len(value_cols),\n",
    "        \",\".join([f\"'{c}', {c}\" for c in value_cols])\n",
    "    )\n",
    "    df_long = df.select(id_col, expr(expr_str))\n",
    "\n",
    "    # 2. Separar Mes y Métrica\n",
    "    df_long = df_long.withColumn(\"Mes\", regexp_extract(\"Columna\", r\"^(.*)_(Venta|TGT|PY24|pct)$\", 1)) \\\n",
    "                     .withColumn(\"Metrica\", regexp_extract(\"Columna\", r\"^(.*)_(Venta|TGT|PY24|pct)$\", 2)) \\\n",
    "                     .drop(\"Columna\")\n",
    "\n",
    "    # 3. Pivotear: convertir Métrica en columnas (formato wide final)\n",
    "    df_wide = df_long.groupBy(id_col, \"Mes\").pivot(\"Metrica\").agg(F.first(\"Valor\"))\n",
    "\n",
    "    # 4. Normalizar columna Mes (limpieza de texto)\n",
    "    df_wide = df_wide.withColumn(\"Mes\", trim(upper(df_wide[\"Mes\"])))\n",
    "\n",
    "    # 5. Limpieza y tipado de métricas (casteo a double y reemplazo de nulos)\n",
    "    metricas = [\"Venta\", \"TGT\", \"PY24\", \"pct\"]\n",
    "    for m in metricas:\n",
    "        df_wide = df_wide.withColumn(\n",
    "            m,\n",
    "            when(col(m).isNull(), 0.0).otherwise(col(m).cast(\"double\"))\n",
    "        )\n",
    "\n",
    "    # 6. Agregar columna de nivel\n",
    "    df_wide = df_wide.withColumn(\"Nivel\", lit(nivel_valor))\n",
    "    \n",
    "    return df_wide\n",
    "\n",
    "# ==============================\n",
    "# 2. Ejecución del Transform\n",
    "# ==============================\n",
    "\n",
    "# Aplicar transformación a ambos datasets\n",
    "df_familia_wide = transform_to_wide_format(df_familia_raw, \"FAMILIA\")\n",
    "df_producto_wide = transform_to_wide_format(df_producto_raw, \"PRODUCTO\")\n",
    "\n",
    "# 3. Unir ambos datasets limpios (Transformación final)\n",
    "df_total = (\n",
    "    df_familia_wide\n",
    "    .unionByName(df_producto_wide, allowMissingColumns=True)\n",
    ")\n",
    "\n",
    "# Muestra para verificación\n",
    "print(\"\\n=== Resultado Unificado y Limpio (después de Transform) ===\")\n",
    "df_total.show(5, truncate=False)\n",
    "df_total.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87f2ae7-11cc-4a23-a7e0-610d1eb27eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Escribiendo resultado limpio en: /user/etl_output/resumen_ventas_limpio.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso ETL (Extract, Transform, Load) finalizado y datos cargados.\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 1. Path de Destino\n",
    "# ==============================\n",
    "output_path = \"/user/etl_output/resumen_ventas_limpio.parquet\"\n",
    "\n",
    "# ==============================\n",
    "# 2. Ejecución del Load\n",
    "# ==============================\n",
    "print(f\"\\nEscribiendo resultado limpio en: {output_path}\")\n",
    "\n",
    "# Escribir el DataFrame final a Parquet en HDFS\n",
    "df_total.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(output_path)\n",
    "\n",
    "print(\"Proceso ETL (Extract, Transform, Load) finalizado y datos cargados.\")\n",
    "\n",
    "# Detener Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7356a79-fa48-4122-bc84-4899948b23ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
