services:
  namenode:
    build: .
    container_name: namenode
    hostname: namenode
    ports:
      - "9870:9870"   # HDFS Web UI
      - "9000:9000"   # HDFS Service (cambiado de 8020)
      - "8042:8042"   # NodeManager Web
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - HDFS_CONF_dfs_replication=2
    volumes:
      - namenode_data:/data/hdfs/namenode
      - ./config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./scripts:/scripts
    command: bash -c "/scripts/start-services-fixed.sh namenode"
    networks:
      - hadoop-network

  datanode1:
    build: .
    container_name: datanode1
    hostname: datanode1
    environment:
      - CLUSTER_NAME=hadoop-cluster
    volumes:
      - datanode1_data:/data/hdfs/datanode
      - ./config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./scripts:/scripts
    command: bash -c "/scripts/start-services-fixed.sh datanode"
    depends_on:
      - namenode
    networks:
      - hadoop-network

  spark-master:
    build: .
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8080:8080"   # Spark Master UI
      - "7077:7077"   # Spark Master
    environment:
      - SPARK_MODE=master
    volumes:
      - ./config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./scripts:/scripts
    command: bash -c "/scripts/start-services-fixed.sh spark-master"
    depends_on:
      - namenode
    networks:
      - hadoop-network

  spark-worker:
    build: .
    container_name: spark-worker
    hostname: spark-worker
    ports:
      - "8081:8081"   # Worker UI
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./scripts:/scripts
    command: bash -c "/scripts/start-services-fixed.sh spark-worker"
    depends_on:
      - spark-master
    networks:
      - hadoop-network

  jupyter:
    build: .
    container_name: jupyter
    hostname: jupyter
    ports:
      - "8888:8888"   # Jupyter Notebook
      - "4040:4040"   # Spark UI
    environment:
      - JUPYTER_ENABLE_LAB=yes
    volumes:
      - ./notebooks:/home/jupyter/notebooks
      - ./config/jupyter_notebook_config.py:/home/jupyter/.jupyter/jupyter_notebook_config.py
      - ./scripts:/scripts
    working_dir: /home/jupyter/notebooks
    command: bash -c "/scripts/start-services-fixed.sh jupyter"
    depends_on:
      - namenode
      - spark-master
    networks:
      - hadoop-network

volumes:
  namenode_data:
  datanode1_data:

networks:
  hadoop-network:
    driver: bridge