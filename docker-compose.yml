services:
  nifi:
    image: apache/nifi:1.23.2
    container_name: nifi
    hostname: nifi
    ports:
      - "8082:8080"   # NiFi UI
    environment:
      - NIFI_WEB_HTTP_PORT=8080
      - NIFI_WEB_HTTP_HOST=0.0.0.0
      - NIFI_CLUSTER_IS_NODE=false
      - NIFI_ZK_CONNECT_STRING=
      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=ctsBtRBKHRAx69EqUghvvgEvjnaLjFEB
    volumes:
      - ./config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - nifi_database:/opt/nifi/nifi-current/database_repository
      - nifi_flowfile:/opt/nifi/nifi-current/flowfile_repository
      - nifi_content:/opt/nifi/nifi-current/content_repository
      - nifi_provenance:/opt/nifi/nifi-current/provenance_repository
      - nifi_state:/opt/nifi/nifi-current/state
      - nifi_logs:/opt/nifi/nifi-current/logs
      - ./data:/opt/nifi/nifi-current/data
    restart: unless-stopped
    depends_on:
      - namenode
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/nifi/"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 120s

  nifi-init:
    image: curlimages/curl:8.9.1
    container_name: nifi-init
    depends_on:
      - nifi
    volumes:
      - ./scripts/init-nifi-template.sh:/init.sh
      - ./template/conexion_nifi_a_hdfs.xml:/conexion_nifi_a_hdfs.xml
    entrypoint: ["sh", "/init.sh"]
    networks:
      - hadoop-network

  namenode:
    build: .
    container_name: namenode
    hostname: namenode
    ports:
      - "9870:9870"   # HDFS Web UI
      - "9000:9000"   # HDFS Service
      - "8042:8042"   # NodeManager Web
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - HDFS_CONF_dfs_replication=2
    volumes:
      - namenode_data:/data/hdfs/namenode
      - ./config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./scripts:/scripts
    command: bash -c "/scripts/start-services-fixed.sh namenode"
    networks:
      - hadoop-network

  datanode1:
    build: .
    container_name: datanode1
    hostname: datanode1
    environment:
      - CLUSTER_NAME=hadoop-cluster
    volumes:
      - datanode1_data:/data/hdfs/datanode
      - ./config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./scripts:/scripts
    command: bash -c "/scripts/start-services-fixed.sh datanode"
    depends_on:
      - namenode
    networks:
      - hadoop-network

  datanode2:
    build: .
    container_name: datanode2
    hostname: datanode2
    environment:
      - CLUSTER_NAME=hadoop-cluster
    volumes:
      - datanode2_data:/data/hdfs/datanode
      - ./config/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./config/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./scripts:/scripts
    command: bash -c "/scripts/start-services-fixed.sh datanode"
    depends_on:
      - namenode
    networks:
      - hadoop-network

  spark-master:
    build: .
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8080:8080"   # Spark Master UI
      - "7077:7077"   # Spark Master
    environment:
      - SPARK_MODE=master
    volumes:
      - ./config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./scripts:/scripts
    command: bash -c "/scripts/start-services-fixed.sh spark-master"
    depends_on:
      - namenode
    networks:
      - hadoop-network

  spark-worker:
    build: .
    container_name: spark-worker
    hostname: spark-worker
    ports:
      - "8081:8081"   # Worker UI
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./scripts:/scripts
    command: bash -c "/scripts/start-services-fixed.sh spark-worker"
    depends_on:
      - spark-master
    networks:
      - hadoop-network

  jupyter:
    build: .
    container_name: jupyter
    hostname: jupyter
    user: root
    ports:
      - "8888:8888"   # Jupyter Notebook
      - "4040:4040"   # Spark UI
    environment:
      - JUPYTER_ENABLE_LAB=yes
    volumes:
      - ./notebooks:/home/jupyter/notebooks
      - ./config/jupyter_notebook_config.py:/home/jupyter/.jupyter/jupyter_notebook_config.py
      - ./scripts:/scripts
    working_dir: /home/jupyter/notebooks
    command: bash -c "/scripts/start-services-fixed.sh jupyter"
    depends_on:
      - namenode
      - spark-master
    networks:
      - hadoop-network

volumes:
  nifi_database:
  nifi_flowfile:
  nifi_content:
  nifi_provenance:
  nifi_state:
  nifi_logs:
  namenode_data:
  datanode1_data:
  datanode2_data:

networks:
  hadoop-network:
    driver: bridge
