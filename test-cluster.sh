#!/bin/bash

echo "ğŸ§ª Iniciando pruebas del cluster Big Data"

# FunciÃ³n para esperar un servicio
wait_for_service() {
    local service_name=$1
    local port=$2
    local max_attempts=30
    local attempt=1
    
    echo "â³ Esperando $service_name en puerto $port..."
    
    while [ $attempt -le $max_attempts ]; do
        if docker exec namenode bash -c "nc -z localhost $port" 2>/dev/null; then
            echo "âœ… $service_name estÃ¡ disponible"
            return 0
        fi
        echo "   Intento $attempt/$max_attempts..."
        sleep 5
        attempt=$((attempt + 1))
    done
    
    echo "âŒ $service_name no estÃ¡ disponible despuÃ©s de $max_attempts intentos"
    return 1
}

echo "ğŸ” Verificando servicios..."

# Verificar que los contenedores estÃ©n ejecutÃ¡ndose
echo "ğŸ“Š Estado de contenedores:"
docker-compose ps

echo ""
echo "ğŸŒ Servicios web disponibles:"
echo "   - Hadoop HDFS UI: http://localhost:9870"
echo "   - Spark Master UI: http://localhost:8080"
echo "   - Spark Worker UI: http://localhost:8081"
echo "   - Jupyter Lab: http://localhost:8888"

# Verificar HDFS
echo ""
echo "ğŸ“ Probando HDFS..."
if docker exec namenode bash -c "hdfs dfs -ls /" 2>/dev/null; then
    echo "âœ… HDFS estÃ¡ operativo"
    
    # Crear directorio de prueba
    docker exec namenode bash -c "hdfs dfs -mkdir -p /test" 2>/dev/null
    echo "âœ… Directorio /test creado en HDFS"
    
    # Subir archivo de prueba
    docker exec namenode bash -c "echo 'Hello Big Data!' | hdfs dfs -put - /test/hello.txt" 2>/dev/null
    echo "âœ… Archivo de prueba subido a HDFS"
    
    # Leer archivo
    echo "ğŸ“– Contenido del archivo:"
    docker exec namenode bash -c "hdfs dfs -cat /test/hello.txt"
else
    echo "âŒ HDFS no estÃ¡ disponible"
fi

# Verificar Spark
echo ""
echo "âš¡ Probando conexiÃ³n a Spark..."
if docker exec spark-master bash -c "nc -z localhost 7077" 2>/dev/null; then
    echo "âœ… Spark Master estÃ¡ disponible"
else
    echo "âŒ Spark Master no estÃ¡ disponible"
fi

echo ""
echo "ğŸ¯ Pruebas completadas. El cluster estÃ¡ listo para usar!"